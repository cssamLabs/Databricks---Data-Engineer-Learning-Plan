{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Engineering Best Practices, DevOps, and CI/CD Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Software Engineering (SWE) Best Practices\n",
    "\n",
    "![swe-best-practices](images/swe-best-practices.png)\n",
    "![swe-best-practices-components](images/swe-best-practices-components.png)\n",
    "![swe-databricks](images/swe-databricks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Modularizing PySpark Code\n",
    "\n",
    "![modularizing-pyspark-benefits](images/modularizing-pyspark-benefits.png)\n",
    "![modularizing-pyspark-code](images/modularizing-pyspark-code.png)\n",
    "![modularizing-pyspark-code-1](images/modularizing-pyspark-code-1.png)\n",
    "![modularizing-pyspark-code-2](images/modularizing-pyspark-code-2.png)\n",
    "![modularizing-pyspark-code-3](images/modularizing-pyspark-code-3.png)\n",
    "![modularizing-pyspark-code-4](images/modularizing-pyspark-code-4.png)\n",
    "![modularizing-pyspark-code-5](images/modularizing-pyspark-code-5.png)\n",
    "![modularizing-pyspark-code-6](images/modularizing-pyspark-code-6.png)\n",
    "![modularizing-pyspark-code-7](images/modularizing-pyspark-code-7.png)\n",
    "![modularizing-pyspark-code-8](images/modularizing-pyspark-code-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DevOps Fundamentals\n",
    "\n",
    "![devops-fundamentals](images/devops-fundamentals.png)\n",
    "![devops-life-cycle-components](images/devops-life-cycle-components.png)\n",
    "![devops-life-cycle-process](images/devops-life-cycle-process.png)\n",
    "![devops-data-engineering-machine-learning](images/devops-data-engineering-machine-learning.png)\n",
    "![devops-dataops-mlops](images/devops-dataops-mlops.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Role of CI/CD in DevOps\n",
    "\n",
    "![role-cicd](images/role-cicd.png)\n",
    "![role-cicd-devops](images/role-cicd-devops.png)\n",
    "![role-cicd-devops-ci](images/role-cicd-devops-ci.png)\n",
    "![role-cicd-devops-cd.png](images/role-cicd-devops-cd.png)\n",
    "![role-cicd-workflow](images/role-cicd-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning the Project\n",
    "\n",
    "![planning-the-project-requirements](images/planning-the-project-requirements.png)\n",
    "![planning-the-project-data-environments](images/planning-the-project-data-environments.png)\n",
    "![planning-the-project-isolating-environments](images/planning-the-project-isolating-environments.png)\n",
    "![planning-the-project-workspace-isolation](images/planning-the-project-workspace-isolation.png)\n",
    "![planning-the-project-unity-catalog-isolation](images/planning-the-project-unity-catalog-isolation.png)\n",
    "![planning-the-project-archtecture](images/planning-the-project-archtecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Unit Tests for PySpark\n",
    "![unit-tests-PySpark-introduction](images/unit-tests-PySpark-introduction.png)\n",
    "![unit-tests-PySpark-utils](images/unit-tests-PySpark-utils.png)\n",
    "![unit-tests-PySpark-example](images/unit-tests-PySpark-example.png)\n",
    "![unit-tests-PySpark-goal](images/unit-tests-PySpark-goal.png)\n",
    "![unit-tests-PyTest-framework](images/unit-tests-PyTest-framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Executing Unit Tests\n",
    "![creating-and-executing-1](images/creating-and-executing-1.png)\n",
    "![creating-and-executing-2](images/creating-and-executing-2.png)\n",
    "![creating-and-executing-3](images/creating-and-executing-3.png)\n",
    "![creating-and-executing-unit-tests](images/creating-and-executing-unit-tests.png)\n",
    "![creating-and-executing-unit-tests-1](images/creating-and-executing-unit-tests-1.png)\n",
    "![creating-and-executing-unit-tests-2](images/creating-and-executing-unit-tests-2.png)\n",
    "![creating-and-executing-unit-tests-3](images/creating-and-executing-unit-tests-3.png)\n",
    "![creating-and-executing-unit-tests-4](images/creating-and-executing-unit-tests-4.png)\n",
    "![creating-and-executing-unit-tests-5](images/creating-and-executing-unit-tests-5.png)\n",
    "![creating-and-executing-unit-tests-6](images/creating-and-executing-unit-tests-6.png)\n",
    "![creating-and-executing-unit-tests-7](images/creating-and-executing-unit-tests-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Integration Tests with DLT and Workflows\n",
    "\n",
    "![executing-integration-test-types](images/executing-integration-test-types.png)\n",
    "![executing-integration-test-dlt](images/executing-integration-test-dlt.png)\n",
    "![executing-integration-test-workflow](images/executing-integration-test-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Integration Tests with DLT and Workflows\n",
    "\n",
    "![integration-with-workflow](images/integration-with-workflow.png)\n",
    "![integration-with-workflow-dlt-pipeline](images/integration-with-workflow-dlt-pipeline.png)\n",
    "![integration-with-workflow-dlt-pipeline-settings](images/integration-with-workflow-dlt-pipeline-settings.png)\n",
    "![integration-with-workflow-dlt-pipeline-settings-2](images/integration-with-workflow-dlt-pipeline-settings-2.png)\n",
    "![integration-with-workflow-dlt-pipeline-settings-yaml](images/integration-with-workflow-dlt-pipeline-settings-yaml.png)\n",
    "![integration-with-workflow-dlt-pipeline-details](images/integration-with-workflow-dlt-pipeline-details.png)\n",
    "![integration-with-workflow-dlt-pipeline-configuration](images/integration-with-workflow-dlt-pipeline-configuration.png)\n",
    "![integration-with-workflow-dlt-pipeline-ingest](images/integration-with-workflow-dlt-pipeline-ingest.png)\n",
    "![integration-with-workflow-dlt-pipeline-silver](images/integration-with-workflow-dlt-pipeline-silver.png)\n",
    "![integration-with-workflow-dlt-pipeline-gold](images/integration-with-workflow-dlt-pipeline-gold.png)\n",
    "![integration-with-workflow-dlt-pipeline-dictionary-create](images/integration-with-workflow-dlt-pipeline-dictionary-create.png)\n",
    "![integration-with-workflow-dlt-pipeline-dictionary-materialized-view](images/integration-with-workflow-dlt-pipeline-dictionary-materialized-view.png)\n",
    "![integration-with-workflow-dlt-pipeline-execute-integration-test](images/integration-with-workflow-dlt-pipeline-execute-integration-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Control with Git Overview\n",
    "\n",
    "Git version control is important because it provides a structured way to manage, track, and collaborate on code changes in software projects.  Essentially, it is a tool for modern software development and DevOps.\n",
    "\n",
    "### Complications with Version Control\n",
    "\n",
    "Let's look at complications with version control. A lack of centralized version control leads to development silos, which results in duplicate code, inconsistent standards, and lower software quality.  Unstable versioning makes tracking, reverting, and auditing changes very difficult, increasing risks when managing frequent updates. Additionally, branching, merging, and CICD integration become challenging. Ultimately, this is all limiting for scalability.\n",
    "\n",
    "### Secure code changes through branching\n",
    "\n",
    "Starting with the first version, v0. 1, on the main branch, branching occurs to the feature branch where testing can be properly performed in isolation.\n",
    "\n",
    "![secure-code-changes-through-branching](images/secure-code-changes-through-branching.png)\n",
    "\n",
    "\n",
    "### Overview of Git with Databricks\n",
    "\n",
    "Git is a free and open source software framework that's designed to track changes in source code during your software development phase.\n",
    "\n",
    "![overview-of-git-with-databricks](images/overview-of-git-with-databricks.png)\n",
    "\n",
    "\n",
    "Version control enables tracking code changes, facilitating rollback and collaboration, while branching and merging allow multiple developers to work in parallel and integrate changes efficiently.  A distributed workflow ensures each developer has a full local repository, which enhances flexibility and reliability. Moreover, Git is optimized for high-performance handling of large projects, and security features use cryptographic integrity checks to prevent data corruption.\n",
    "\n",
    "Users can leverage remote Git repos while developing code inside of Databricks notebooks,  and the repo's REST API enables integration of data and AI projects into CICD pipelines, allowing users to automate Git workflows.\n",
    "\n",
    "And lastly, you should always be strategic about how you're integrating third party tools into your workflows and use whatever makes sense for your organizational needs. \n",
    "\n",
    "\n",
    "\n",
    "### Connecting to Databricks with GitHub PAT\n",
    "\n",
    "Once you have the proper permissions configured for your GitHub repository, go ahead and copy the PAT and head on over to Databricks.\n",
    "\n",
    "![connecting-to-databricks-with-gitHub-PAT](images/connecting-to-databricks-with-gitHub-PAT.png)\n",
    "\n",
    "\n",
    "### Git-Based Repos in Databricks\n",
    "\n",
    "\n",
    "The Repos API provides programmatic access to Git-based repos. And with the API, you can integrate Databricks repos with your CICD workflow.\n",
    "\n",
    "You can programmatically create, update, and delete repos, perform Git operations, and specify Git versions when running jobs based on notebooks within the repo.  \n",
    "\n",
    "![git-folders-databricks](images/git-folders-databricks.png)\n",
    "\n",
    "![git-based-repos-in-databricks](images/git-based-repos-in-databricks.png)\n",
    "\n",
    "### Arbitrary Files Support in Repos\n",
    "![arbitrary-files-support-in-repos](images/arbitrary-files-support-in-repos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Continuous Deployment (CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Databricks Assets Overview\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "When thinking of deploying Databricks assets, we have three different developer options. We have the REST API, the Databricks CLI, and the Databricks SDK.  \n",
    "\n",
    "The REST API is most flexible, but complex, and this is best for custom integrations. The CLI simplifies REST API operations, but has limited flexibility.\n",
    "\n",
    "Finally, the SDK is the most developer-friendly and is best used for embedding Databricks functionality within applications.\n",
    "\n",
    "![deployment-options](images/deployment-options.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks Asset Bundles\n",
    "\n",
    "Databricks Asset Bundles, or DABs, are designed to facilitate the adoption of best practices in software engineering, particularly for data and AI projects.  Here, we will identify four core components of software engineering practices that are supported by DABs.\n",
    "\n",
    "![databricks-asset-bundles](images/databricks-asset-bundles.png)\n",
    "\n",
    "Databricks asset bundles provide a structured approach to managing Databricks projects while adhering to software engineering best practices. By combining infrastructure as code principles with automation capabilities, they streamline collaboration, improve quality assurance, and enable efficient delivery of data-driven solutions.\n",
    "\n",
    "DABS integrates seamlessly with Git-based workflows, enabling users to version their Databricks resources alongside source code.  By treating Databricks resources as code, DABS enables peer review through standard Git workflows like pull requests. Developers can use the Databricks  CLI with DABS to run tests on bundles and isolated environments. This ensures that workflows behave as intended.  \n",
    "\n",
    "Finally, DABS integrates with CICD tools like GitHub Actions or Azure DevOps to automate validation, deployment, and execution on Databricks workflows.  With this foundation in place, let's take a closer look at what DABs are.  \n",
    "\n",
    "![dbas](images/DBAs.png)\n",
    "\n",
    "With DABs, you can create code that can be deployed across multiple environments without modification. This ensures consistency, reduces manual errors, and accelerates delivery by automating deployment processes.\n",
    "\n",
    "DABs are a tool designed to streamline this process for Databricks projects. They enable developers to define Databricks resources like jobs, pipelines, and notebooks as source files and metadata in YAML format.\n",
    "\n",
    "Dabs work by first defining your resources and requirements in a Databricks. yaml file. You then validate the bundle utilizing the Databricks CLI and deploy it to your chosen workspace. Once deployed, workflows or pipelines described in the bundle can be executed.\n",
    "\n",
    "\n",
    "### Development and CI/CD with DABs\n",
    "\n",
    "![development-and-CI/CD-with-DABs](images/development-and-CI-CD-with-DABs.png)\n",
    "\n",
    "To summarize, Databricks Asset Bundles are a tool designed to simplify the management and deployment of data and AI projects on the Databricks platform.  They follow an infrastructure as code approach, allowing users to define and manage Databricks resources such as jobs, pipelines, notebooks, and machine learning models through YAML configuration files.\n",
    "\n",
    "These bundles streamline collaboration, testing, deployment, and version control across various environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ci-cd-pipeline](images/ci-cd-pipeline.png)\n",
    "![ci-cd-pipeline-job](images/ci-cd-pipeline-job.png)\n",
    "![ci-cd-pipeline-job-workflow](images/ci-cd-pipeline-job-workflow.png)\n",
    "![ci-cd-pipeline-job-workflow-delta](images/ci-cd-pipeline-job-workflow-delta.png)\n",
    "![ci-cd-pipeline-job-workflow-visualization](images/ci-cd-pipeline-job-workflow-visualization.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
